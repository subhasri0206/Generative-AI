import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, random_split

# Step 1: Prepare Your Data
texts = ["DistilBERT is amazing!", "I am not happy with this service.", "I really love DistilBert", "Horrible experience!", "Great service!"]
labels = [1, 0, 1, 0, 1]  # Binary labels for positive (1) and negative (0) sentiment

# Load the pre-trained DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the data
input_ids = []
attention_masks = []

for text in texts:
    encoded_dict = tokenizer.encode_plus(
        text,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        max_length=64,            # Pad & truncate all sentences
        padding='max_length',
        truncation=True,
        return_attention_mask=True,   # Construct attention masks
        return_tensors='pt',          # Return PyTorch tensors
    )
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

# Convert lists to tensors
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)

# Step 2: Create DataLoader for Training and Validation
dataset = TensorDataset(input_ids, attention_masks, labels)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=2)

# Step 3: Fine-tune the DistilBERT model
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Define the optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Set up training loop
epochs = 50

for epoch in range(epochs):
    print(f'Epoch {epoch + 1}/{epochs}')

    # Training
    model.train()
    total_loss = 0
    for batch in train_dataloader:
        b_input_ids, b_attention_mask, b_labels = batch

        optimizer.zero_grad()

        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f"Training loss: {avg_train_loss:.4f}")

    # Validation
    model.eval()
    total_eval_accuracy = 0
    total_eval_loss = 0
    for batch in val_dataloader:
        b_input_ids, b_attention_mask, b_labels = batch

        with torch.no_grad():
            outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)

        loss = outputs.loss
        logits = outputs.logits

        total_eval_loss += loss.item()

        preds = torch.argmax(logits, dim=1).flatten()
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        total_eval_accuracy += accuracy

    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)
    avg_val_loss = total_eval_loss / len(val_dataloader)

    print(f"Validation Loss: {avg_val_loss:.2f}")
    print(f"Validation Accuracy: {avg_val_accuracy:.2f}%\n")

# Step 4: Use the model for predictions
model.eval()

texts_to_classify = ["Great DistilBert", "This was a terrible experience."]
input_ids = []
attention_masks = []

for text in texts_to_classify:
    encoded_dict = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=64,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt',
    )
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)

with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_masks)
    logits = outputs.logits

predicted_labels = torch.argmax(logits, dim=1).numpy()
for text, label in zip(texts_to_classify, predicted_labels):
    sentiment = 'Positive' if label == 1 else 'Negative'
    print(f"Text: {text} | Sentiment: {sentiment}")

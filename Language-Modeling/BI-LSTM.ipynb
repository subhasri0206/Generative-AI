# Bi-LSTM (Bidirectional LSTM)
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Define the training data
training_data = [
    "The quick brown fox jumps over the lazy dog.",
    "The sun is shining brightly in the clear blue sky.",
    "The cat purrs contentedly on my lap.",
    "The dog is very happy to see its owner.",
    #...
]

# Create a tokenizer
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(training_data)

# Convert the text data to sequences
sequences = tokenizer.texts_to_sequences(training_data)

# Pad the sequences
max_length = 10
padded_sequences = pad_sequences(sequences, maxlen=max_length)

# Create a dataset of input and output sequences
input_sequences = []
output_sequences = []
for sequence in padded_sequences:
    for i in range(1, len(sequence)):
        input_sequences.append(sequence[:i])
        output_sequences.append(sequence[i])

# Pad the input sequences to a uniform length
input_sequences = pad_sequences(input_sequences, maxlen=max_length - 1)

# One-hot encode the output sequences
num_classes = len(set(word for sequence in padded_sequences for word in sequence))
output_sequences = tf.keras.utils.to_categorical(output_sequences, num_classes)

# Define the bidirectional LSTM model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_length-1))
model.add(Bidirectional(LSTM(128, return_sequences=False)))
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(input_sequences, output_sequences, epochs=100, batch_size=32)

# Define a function to generate remaining words
def generate_remaining_words(model, tokenizer, input_sequence, num_words):
    generated_words = []
    for _ in range(num_words):
        input_sequence = pad_sequences([input_sequence], maxlen=max_length - 1)
        output = model.predict(input_sequence)
        output_index = np.argmax(output[0])
        generated_word = tokenizer.index_word[output_index]
        generated_words.append(generated_word)
        input_sequence = input_sequence[0].tolist()
        input_sequence.append(output_index)
        input_sequence = input_sequence[1:]  # Slide window forward
    return generated_words

# Test the model
input_sequence = tokenizer.texts_to_sequences(["The cat purrs contentedly"])[0]
remaining_words = generate_remaining_words(model, tokenizer, input_sequence, 3)
print("The remaining words are:", ' '.join(remaining_words))
